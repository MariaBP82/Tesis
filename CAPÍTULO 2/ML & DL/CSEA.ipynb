{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9a9c8-2af5-4a77-999e-f263b75860b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import community as community_louvain\n",
    "\n",
    "# --- Paso 1: Algoritmo k-truss ---\n",
    "def k_truss(graph, k):\n",
    "    G = graph.copy()\n",
    "    while True:\n",
    "        edges_to_remove = []\n",
    "        for u, v in G.edges():\n",
    "            common_neighbors = set(G.neighbors(u)).intersection(G.neighbors(v))\n",
    "            if len(common_neighbors) < k - 2:\n",
    "                edges_to_remove.append((u, v))\n",
    "        if not edges_to_remove:\n",
    "            break\n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "    return G\n",
    "\n",
    "# --- Paso 2: Autoencoder Variacional ---\n",
    "class VariationalAutoencoder(Model):\n",
    "    def __init__(self, original_dim, latent_dim, num_clusters):\n",
    "        super().__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(original_dim,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(latent_dim * 2),  # mean and logvar concatenados\n",
    "        ])\n",
    "        self.decoder_reconstruction = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(original_dim, activation='sigmoid'),\n",
    "        ])\n",
    "        self.decoder_classification = tf.keras.Sequential([\n",
    "            layers.InputLayer(shape=(latent_dim,)),\n",
    "            layers.Dense(num_clusters, activation='softmax'),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        mean_logvar = self.encoder(x)\n",
    "        mean, logvar = tf.split(mean_logvar, num_or_size_splits=2, axis=1)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        reconstructed = self.decoder_reconstruction(z)\n",
    "        classification = self.decoder_classification(z)\n",
    "        return reconstructed, classification, mean, logvar\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mean, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mean))\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "def vae_loss(x, reconstructed, classification, mean, logvar, true_labels_onehot):\n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(x, reconstructed))\n",
    "    kl_divergence = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "    classification_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(true_labels_onehot, classification))\n",
    "    return reconstruction_loss + kl_divergence + classification_loss\n",
    "\n",
    "# --- Algoritmo CSEA ---\n",
    "def csea_algorithm(graph, latent_dim=10, num_epochs=100, true_labels=None):\n",
    "    # Obtener etiquetas con Louvain si no se dan\n",
    "    if true_labels is None:\n",
    "        partition = community_louvain.best_partition(graph, weight='weight')\n",
    "        true_labels = np.array([partition[node] for node in sorted(graph.nodes())])\n",
    "    num_clusters = len(np.unique(true_labels))\n",
    "    latent_dim = min(latent_dim, num_clusters * 2)\n",
    "\n",
    "    # Paso 1: k-truss para similitud\n",
    "    k_truss_graph = k_truss(graph, k=3)\n",
    "    adjacency_matrix = nx.to_numpy_array(k_truss_graph, nodelist=sorted(graph.nodes()))\n",
    "    similarity_matrix = normalize(adjacency_matrix + np.eye(adjacency_matrix.shape[0]))\n",
    "\n",
    "    # One-hot para etiquetas\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    true_labels_onehot = encoder.fit_transform(true_labels.reshape(-1, 1))\n",
    "\n",
    "    # Crear modelo VAE\n",
    "    vae = VariationalAutoencoder(similarity_matrix.shape[1], latent_dim, num_clusters)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed, classification, mean, logvar = vae(similarity_matrix.astype(np.float32))\n",
    "            loss = vae_loss(similarity_matrix.astype(np.float32), reconstructed, classification, mean, logvar, true_labels_onehot)\n",
    "        grads = tape.gradient(loss, vae.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.numpy():.4f}\")\n",
    "\n",
    "    # Obtener embedding latente\n",
    "    mean_logvar = vae.encoder(similarity_matrix.astype(np.float32))\n",
    "    mean, _ = tf.split(mean_logvar, 2, axis=1)\n",
    "    latent_features = mean.numpy()\n",
    "\n",
    "    # KMeans sobre embedding\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(latent_features)\n",
    "\n",
    "    return cluster_labels, latent_features\n",
    "\n",
    "# --- EJEMPLO DE USO ---\n",
    "\n",
    "# Crear grafo de ejemplo\n",
    "G = nx.Graph()\n",
    "edges = [(0,1),(1,2),(2,0),  # triÃ¡ngulo\n",
    "         (2,3),(3,4),(4,5),(5,3)]  # cuadrado conectado\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Ejecutar CSEA\n",
    "labels, embeddings = csea_algorithm(G, latent_dim=4, num_epochs=100)\n",
    "\n",
    "print(\"\\nEtiquetas de comunidades asignadas:\")\n",
    "for node, label in zip(sorted(G.nodes()), labels):\n",
    "    print(f\"Nodo {node}: Comunidad {label}\")\n",
    "\n",
    "print(\"\\nEmbedding latente de cada nodo:\")\n",
    "print(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
